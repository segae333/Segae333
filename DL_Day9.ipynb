{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "DL_Day9.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Jf1KbbeqoXbt"
      },
      "source": [
        "### 단어로 감성 분류하기"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Yq0HWsZ6fZph"
      },
      "source": [
        "import tensorflow as tf\n",
        "from tensorflow.keras import layers\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from IPython.display import clear_output\n",
        "\n",
        "import time"
      ],
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yjTua09VoeCM"
      },
      "source": [
        "### 데이터셋 구성\n",
        "- 각 단어에 대한 정답을 부정(0), 긍정(1)로 정의해서 데이터셋을 구성\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ir5tYYZlojJq"
      },
      "source": [
        "x_train_words = ['good', 'bad', 'amazing', 'so good', 'bull shit',\n",
        "                 'awesome', 'how dare', 'very much', 'nice', 'god damn it',\n",
        "                 'very very very happy', 'what the fuck']\n",
        "y_train = np.array([1, 0, 1, 1, 0,\n",
        "                    1, 0, 1, 1, 0,\n",
        "                    1, 0], dtype=np.int32)"
      ],
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zt4ZhZSpoj7L",
        "outputId": "bfa265b0-e7d8-4c30-d71c-ee5638b1f373"
      },
      "source": [
        "# negative sample\n",
        "index = 1\n",
        "print(\"word: {}\\nlabel: {}\".format(x_train_words[index], y_train[index]))"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "word: bad\n",
            "label: 0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PkFNkM47oj4r",
        "outputId": "6ed35177-d12b-4c6f-fc17-67683fa0419f"
      },
      "source": [
        "# positive sample\n",
        "index = 0\n",
        "print(\"word: {}\\nlabel: {}\".format(x_train_words[index], y_train[index]))"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "word: good\n",
            "label: 1\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IOAGKoWTorMk"
      },
      "source": [
        "### 텍스트데이터 처리를 위한 Tokenizer사용"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2OfEjOnJoj2C"
      },
      "source": [
        "from tensorflow.python.keras.preprocessing.text import Tokenizer\n",
        "from tensorflow.python.keras.preprocessing.sequence import pad_sequences"
      ],
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aAmS08W8ojzV"
      },
      "source": [
        "tokenizer = Tokenizer(char_level=True)"
      ],
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Sx-B8GhOojwx"
      },
      "source": [
        "tokenizer.fit_on_texts(x_train_words)"
      ],
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Xq5oQj4eojt5",
        "outputId": "bb56bf86-95a1-4d09-d0b9-aeb6db2bdf92"
      },
      "source": [
        "num_chars = len(tokenizer.word_index) + 1\n",
        "print(\"number of characters: {}\".format(num_chars))"
      ],
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "number of characters: 25\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "37c26hJfojrH",
        "outputId": "3bef6dd9-4f1a-46a3-a05d-9a300f81a5e4"
      },
      "source": [
        "tokenizer.word_index"
      ],
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{' ': 1,\n",
              " 'a': 4,\n",
              " 'b': 19,\n",
              " 'c': 18,\n",
              " 'd': 5,\n",
              " 'e': 2,\n",
              " 'f': 23,\n",
              " 'g': 9,\n",
              " 'h': 6,\n",
              " 'i': 11,\n",
              " 'k': 24,\n",
              " 'l': 20,\n",
              " 'm': 10,\n",
              " 'n': 14,\n",
              " 'o': 3,\n",
              " 'p': 21,\n",
              " 'r': 7,\n",
              " 's': 15,\n",
              " 't': 12,\n",
              " 'u': 16,\n",
              " 'v': 13,\n",
              " 'w': 17,\n",
              " 'y': 8,\n",
              " 'z': 22}"
            ]
          },
          "metadata": {},
          "execution_count": 9
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zelh8qdqojoY"
      },
      "source": [
        "x_train_tokens = tokenizer.texts_to_sequences(x_train_words)"
      ],
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UvjwaaqWozc5",
        "outputId": "10695f99-670d-44dc-c9cf-0e96c90887c6"
      },
      "source": [
        "index = 2\n",
        "print(\"text: {}\".format(x_train_words[index]))\n",
        "print(\"token: {}\".format(x_train_tokens[index]))"
      ],
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "text: amazing\n",
            "token: [4, 10, 4, 22, 11, 14, 9]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ddpJA_LlozaX"
      },
      "source": [
        "x_train_seq_length = np.array([len(tokens) for tokens in x_train_tokens], dtype=np.int32)\n",
        "num_seq_length = x_train_seq_length"
      ],
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hH-o0oXKozXi",
        "outputId": "43f9c368-c1ff-4606-e390-b3f480d2dba1"
      },
      "source": [
        "max_seq_length = np.max(num_seq_length)\n",
        "print(max_seq_length)"
      ],
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "20\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CN_YxMauozU7"
      },
      "source": [
        "pad = 'pre'\n",
        "# pad = 'post'"
      ],
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CnYkDEZoozNZ"
      },
      "source": [
        "x_train_pad = pad_sequences(sequences=x_train_tokens, maxlen=max_seq_length,\n",
        "                            padding=pad, truncating=pad)"
      ],
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "aiFhi3e7o6KJ",
        "outputId": "73b48a3c-83e4-490d-bbc7-7d79900e0c37"
      },
      "source": [
        "index = 7\n",
        "print(\"text : {}\\n\".format(x_train_words[index]))\n",
        "print(\"token : {}\\n\".format(x_train_tokens[index]))\n",
        "print(\"pad: {}\".format(x_train_pad[index]))"
      ],
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "text : very much\n",
            "\n",
            "token : [13, 2, 7, 8, 1, 10, 16, 18, 6]\n",
            "\n",
            "pad: [ 0  0  0  0  0  0  0  0  0  0  0 13  2  7  8  1 10 16 18  6]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "txvCqurEo6Hd",
        "outputId": "a1bd7af7-2018-4322-b549-aa8f6a4bbf0e"
      },
      "source": [
        "idx = tokenizer.word_index\n",
        "inverse_map = dict(zip(idx.values(), idx.keys()))\n",
        "print(inverse_map)"
      ],
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{1: ' ', 2: 'e', 3: 'o', 4: 'a', 5: 'd', 6: 'h', 7: 'r', 8: 'y', 9: 'g', 10: 'm', 11: 'i', 12: 't', 13: 'v', 14: 'n', 15: 's', 16: 'u', 17: 'w', 18: 'c', 19: 'b', 20: 'l', 21: 'p', 22: 'z', 23: 'f', 24: 'k'}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_hz4OrsYo6Ew"
      },
      "source": [
        "def tokens_to_string(tokens):\n",
        "    words = [inverse_map[token] for token in tokens if token != 0]\n",
        "\n",
        "    text = \"\".join(words)\n",
        "\n",
        "    return text"
      ],
      "execution_count": 18,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6WaL-3Rxo6CV",
        "outputId": "94aa621e-4fe0-486d-b0e6-fcaf15cf542d"
      },
      "source": [
        "index = 10\n",
        "print(\"original text : \\n{}\\n\".format(x_train_words[index]))\n",
        "print(\"tokens: \\n{}\\n\".format(x_train_tokens[index]))\n",
        "print(\"tokens to string: \\n{}\".format(tokens_to_string(x_train_tokens[index])))"
      ],
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "original text : \n",
            "very very very happy\n",
            "\n",
            "tokens: \n",
            "[13, 2, 7, 8, 1, 13, 2, 7, 8, 1, 13, 2, 7, 8, 1, 6, 4, 21, 21, 8]\n",
            "\n",
            "tokens to string: \n",
            "very very very happy\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0mmFInyeo5_a"
      },
      "source": [
        "batch_size = 4\n",
        "max_epochs = 50\n",
        "num_units = 16\n",
        "num_classes = 2 \n",
        "initializer_scale = 0.1\n",
        "learning_rate = 1e-3"
      ],
      "execution_count": 20,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "W20G3DUXo58v",
        "outputId": "ac434335-a62e-416f-ebd0-f102b5aa8307"
      },
      "source": [
        "# tf.data로 data pipline 생성\n",
        "train_dataset = tf.data.Dataset.from_tensor_slices((x_train_pad, x_train_seq_length, y_train))\n",
        "train_dataset = train_dataset.shuffle(buffer_size = 100)\n",
        "train_dataset = train_dataset.repeat()\n",
        "train_dataset = train_dataset.batch(batch_size = batch_size)\n",
        "print(train_dataset)"
      ],
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "<BatchDataset shapes: ((None, 20), (None,), (None,)), types: (tf.int32, tf.int32, tf.int32)>\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Y443sKYBpC3r"
      },
      "source": [
        "model = tf.keras.Sequential([\n",
        "            layers.Embedding(num_chars, num_chars, embeddings_initializer='identity', trainable=False),\n",
        "            layers.SimpleRNN(units=num_units),\n",
        "            layers.Dense(units=num_classes, activation='sigmoid')\n",
        "])"
      ],
      "execution_count": 24,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xZK8126GpC1R"
      },
      "source": [
        "optimizer = tf.keras.optimizers.Adam(learning_rate)\n",
        "loss_obj = tf.keras.losses.BinaryCrossentropy(from_logits=False)\n",
        "mean_loss = tf.keras.metrics.Mean(\"loss\")\n",
        "loss_history = []"
      ],
      "execution_count": 25,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xei9f2R4pics"
      },
      "source": [
        "### tf.GradientTape을 이용한 학습 진행"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JQS4gfbEpCys",
        "outputId": "9a0d03c1-cfa2-401b-cb21-f7934d5fd5fb"
      },
      "source": [
        "total_steps = int(len(x_train_words)/ batch_size * max_epochs)\n",
        "\n",
        "for (step, (seq_pad, seq_length, labels)) in enumerate(train_dataset.take(total_steps)):\n",
        "    start_time = time.time()\n",
        "    with tf.GradientTape() as tape:\n",
        "        logits = model(seq_pad)\n",
        "        loss_value = loss_obj(tf.one_hot(labels, depth=num_classes), logits)\n",
        "\n",
        "    mean_loss(loss_value)\n",
        "    loss_history.append((mean_loss.result().numpy()))\n",
        "    grads = tape.gradient(loss_value, model.variables)\n",
        "    optimizer.apply_gradients(zip(grads, model.variables))\n",
        "\n",
        "    if step % 3 == 0:\n",
        "        clear_output(wait=True)\n",
        "        duration = time.time() - start_time\n",
        "        examples_per_sec = batch_size/float(duration)\n",
        "        epochs = batch_size * step / float(len(x_train_words))\n",
        "        print(\"epochs : {:.2f}, step : {}, loss: {:g}, ({:.2f} examples/sec; {: .3f} sec/batch\".format(epochs+1, step, loss_value, examples_per_sec, duration))\n",
        "print(\"training done!!!!\")"
      ],
      "execution_count": 27,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "epochs : 50.00, step : 147, loss: 0.0615634, (135.28 examples/sec;  0.030 sec/batch\n",
            "training done!!!!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TqnE-z1ppCwV"
      },
      "source": [
        "loss_history = np.array(loss_history)\n",
        "plt.plot(loss_history, label='train')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FEVWTpJ9qyua"
      },
      "source": [
        "### 모델 평가"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lwftXDBmpCts"
      },
      "source": [
        "train_dataset_eval = tf.data.Dataset.from_tensor_slices((x_train_pad, x_train_seq_length, y_train))\n",
        "train_dataset_eval = train_dataset_eval.batch(batch_size=len(x_train_pad))"
      ],
      "execution_count": 28,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FrVXzRzDq17K"
      },
      "source": [
        "loss_object = tf.keras.losses.CategoricalCrossentropy()\n",
        "acc_object = tf.keras.metrics.CategoricalAccuracy()\n",
        "val_acc_object = tf.keras.metrics.CategoricalAccuracy()"
      ],
      "execution_count": 29,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VxfT2i-_q14d"
      },
      "source": [
        "val_mean_loss = tf.keras.metrics.Mean(\"loss\")\n",
        "val_mean_accuracy = tf.keras.metrics.Mean(\"accuracy\")"
      ],
      "execution_count": 30,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5voxAbUkq11_",
        "outputId": "38fbdd6b-aa18-440f-bb37-efad589361eb"
      },
      "source": [
        "for (step, (seq_pad, seq_length, labels)) in enumerate(train_dataset.take(1)):\n",
        "    predictions = model(seq_pad, training=False)\n",
        "    val_loss_value = loss_object(tf.one_hot(labels, depth=num_classes), predictions)\n",
        "    val_acc_value = val_acc_object(tf.one_hot(labels, depth=num_classes), predictions)\n",
        "\n",
        "    val_mean_loss(val_loss_value)\n",
        "    val_mean_accuracy(val_acc_value)\n",
        "\n",
        "    print(\"valid loss : {: .4g}, valid accuracy : {: .4g}%\".format(val_mean_loss.result(),\n",
        "                                                                   val_mean_accuracy.result() * 100))"
      ],
      "execution_count": 31,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "valid loss :  0.004676, valid accuracy :  100%\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7QopZP5pq1zc"
      },
      "source": [
        "for (step, (seq_pad, seq_length, labels)) in enumerate(train_dataset_eval.take(1)):\n",
        "    logits = model(seq_pad)\n",
        "    predictions = tf.cast(tf.argmax(logits, 1), tf.int32)"
      ],
      "execution_count": 32,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "iGsexZVLwjpi",
        "outputId": "dd6fccb4-55d5-4a49-a394-7bc1e9bbc99a"
      },
      "source": [
        "predictions"
      ],
      "execution_count": 33,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<tf.Tensor: shape=(12,), dtype=int32, numpy=array([1, 0, 1, 1, 0, 1, 0, 1, 1, 0, 1, 0], dtype=int32)>"
            ]
          },
          "metadata": {},
          "execution_count": 33
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pOvJZU9GwjkP",
        "outputId": "11be8f63-1e35-468a-f594-c6865eedca72"
      },
      "source": [
        "for x , y in zip(seq_pad, predictions):\n",
        "    if y.numpy() == 1:\n",
        "        print(\"{}: positive\".format(tokens_to_string(x.numpy())))\n",
        "    else:\n",
        "        print(\"{}: negative\".format(tokens_to_string(x.numpy())))"
      ],
      "execution_count": 34,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "good: positive\n",
            "bad: negative\n",
            "amazing: positive\n",
            "so good: positive\n",
            "bull shit: negative\n",
            "awesome: positive\n",
            "how dare: negative\n",
            "very much: positive\n",
            "nice: positive\n",
            "god damn it: negative\n",
            "very very very happy: positive\n",
            "what the fuck: negative\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oJTJBcCawjfh"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QQTa-4jrwjcP"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}